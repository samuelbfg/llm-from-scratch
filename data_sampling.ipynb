{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc65260e",
   "metadata": {},
   "source": [
    "We need to generate the input-target pairs required for training an LLM\n",
    "\n",
    "Goal: implement a data loader that fetches the input-target pairs from the training dataset using a sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4015e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a070397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88630\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/fadat_noticias.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472eb094",
   "metadata": {},
   "source": [
    "## Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1f6982d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 376, 2885, 1404, 2169, 401, 78, 1296, 283, 773]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sample = enc_text[:10]\n",
    "enc_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248f4f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [376, 2885, 1404, 16614]\n",
      "y:      [2885, 1404, 16614, 68]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 # number of tokens in the input sequence\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "906d52fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[376] ----> 2885\n",
      "[376, 2885] ----> 1404\n",
      "[376, 2885, 1404] ----> 16614\n",
      "[376, 2885, 1404, 16614] ----> 68\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17675891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " F ----> AD\n",
      " FAD ----> AT\n",
      " FADAT ---->  pretend\n",
      " FADAT pretend ----> e\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b2c586",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/llm_dataloader.png\" alt=Diagram showing a large language model represented by interconnected neural network nodes processing a sequence of text tokens. The nodes are arranged in layers, illustrating the flow of information from input to output. The workspace is digital and organized, with a neutral and focused atmosphere. No visible text is present in the image. The tone is analytical and educational, emphasizing the complexity and structure of language model data processing. width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0f3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt) # tokenize the entire text\n",
    "        \n",
    "        # sliding window to chunk the text into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride): \n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self): # total number of rows in the dataset\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx): # return a single row from the dataset\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42c075a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "    stride=128, shuffle=True, drop_last=True, num_workers=0\n",
    "    ):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "248026bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  32,  376, 2885, 1404]]), tensor([[ 376, 2885, 1404, 2169]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/fadat_noticias.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader) # dataloader -> iterator\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ec219",
   "metadata": {},
   "source": [
    "Obs: `max_length` is set to 4 but is relatively small, only chosen for illustration purposes. \n",
    "\n",
    "It is common to train LLMs with input sizes of at least 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84e863ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  32,  376, 2885, 1404]]), tensor([[ 376, 2885, 1404, 2169]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "batch = next(data_iter)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cd3a6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  795,    79,  2301, 14991],\n",
      "        [  267,   269,  5669,  8873],\n",
      "        [44349, 10034,    64,    13],\n",
      "        [   78,   384,  6592,  2418],\n",
      "        [ 2319, 13276,    11,  2448],\n",
      "        [13161,  4533, 31215,   257],\n",
      "        [33380,   283, 38251,   435],\n",
      "        [  390,   755,  8836, 13370]])\n",
      "\n",
      "Targets:\n",
      " tensor([[   79,  2301, 14991,   312],\n",
      "        [  269,  5669,  8873, 31215],\n",
      "        [10034,    64,    13,   198],\n",
      "        [  384,  6592,  2418,  6557],\n",
      "        [13276,    11,  2448, 22019],\n",
      "        [ 4533, 31215,   257,  1296],\n",
      "        [  283, 38251,   435,  2188],\n",
      "        [  755,  8836, 13370,   292]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450757c",
   "metadata": {},
   "source": [
    "## Creating token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36b85bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7e69cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a84387c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9f021",
   "metadata": {},
   "source": [
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row. \n",
    "\n",
    "In other words, the embedding layer is essentially a look-up operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e051a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c9196",
   "metadata": {},
   "source": [
    "This is how embedding vectors are created from token IDs. \n",
    "\n",
    "Let's add a small modification to these embedding vectors to encode positional information about a token within a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bccf21",
   "metadata": {},
   "source": [
    "## Encoding word positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2fa90",
   "metadata": {},
   "source": [
    "Self-attention mechanism doesn't have a notion of position or order for the tokens within a sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7481543",
   "metadata": {},
   "source": [
    "In principle, the deterministic, position-independent embedding of the token ID we coded is good for reproducibility purposes. \n",
    "\n",
    "However, since the self-attention mechanism of LLMs itself is also position-agnostic, it is helpful to inject additional position information into the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6abe41",
   "metadata": {},
   "source": [
    "Absolute positional embeddings are directly associated with specific positions in a sequence. \n",
    "\n",
    "**For each position in the input sequence, a unique embedding is added to the token's embedding to convey its exact location.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a892ba5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/positional_embeddings.png\" alt=Positional embeddings diagram showing colored vectors aligned with a sequence of text tokens. Each vector represents a unique position in the sequence, visually illustrating how positional information is added to token embeddings in a neural network. The background is clean and minimal, emphasizing the technical and educational focus. No visible text is present. The tone is analytical and instructional, supporting understanding of how language models encode order and position. width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b45a6",
   "metadata": {},
   "source": [
    "- Positional embeddings are added to the token embedding vector to create the input embeddings for an LLM.\n",
    "- The positional vectors have the same dimension as the original token embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1189e1",
   "metadata": {},
   "source": [
    "Instead of focusing on the absolute position of a token, the emphasis of relative positional embeddings is on the relative position or distance between tokens. \n",
    "\n",
    "This means the model learns the relationships in terms of \"how far apart\" rather than \"at which exact position.\"\n",
    "\n",
    "The advantage here is that the model can generalize better to sequences of varying lengths, even if it hasn't seen such lengths during training.\n",
    "\n",
    "Both types of positional embeddings aim to **augment the capacity of LLMs to understand the order and relationships between tokens**, ensuring more accurate and context-aware predictions. \n",
    "\n",
    "The choice between them often depends on the specific application and the nature of the data being processed.\n",
    "\n",
    "<u>For example:</u> OpenAI's GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the positional encodings in the original Transformer model.\n",
    "\n",
    "This optimization process is part of the model training itself, which we will implement later in this book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad951ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a697354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   32,   376,  2885,  1404],\n",
      "        [ 2169,   401,    78,  1296],\n",
      "        [  283,   773,   452,  8836],\n",
      "        [  646,   418,  1451, 36096],\n",
      "        [  390,  1323,  7718,   369],\n",
      "        [  258,    66,  3681,   418],\n",
      "        [  304,   390, 17463,   263],\n",
      "        [ 7736,   528,  6557,    12]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "348c5921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3fa94",
   "metadata": {},
   "source": [
    "For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same dimension as the token_embedding_layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94f985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length # represents the supported input size of the LLM.\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4eb46b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dca1c375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
       "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
       "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
       "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7b42d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0308, -0.4387, -1.0033,  ..., -2.0125,  0.3858, -0.8801],\n",
       "         [ 0.0240, -0.7000, -0.2415,  ..., -1.2735, -0.5178,  1.2304],\n",
       "         [-0.2507, -0.0546,  0.6687,  ...,  0.9618,  2.3737, -0.0528],\n",
       "         [-0.3368,  0.9981, -0.5168,  ..., -1.4778,  0.5504, -1.5233]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31dcf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
