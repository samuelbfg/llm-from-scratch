{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b735e2b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/llm_mental_model.png\" alt=\"Diagram illustrating the mental model of a large language model with interconnected nodes representing neural network layers processing text input. The workspace is clean and digital, conveying a neutral and focused atmosphere.\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a80ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/fadat_noticias.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "contents = [item['content'] for item in data if 'content' in item]\n",
    "\n",
    "with open('data/fadat_noticias.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc87c8a",
   "metadata": {},
   "source": [
    "## Steps necessary for preparing the embeddings:\n",
    "\n",
    "1. Splitting text into words;\n",
    "2. Converting words into tokens;\n",
    "3. Turning tokens into embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ea287",
   "metadata": {},
   "source": [
    "### 1. Splitting text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4d5740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 239794\n",
      "A FADAT tem como formar indiv√≠duos capazes de buscar conhecimentos e de saber utiliz√°-los.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/fadat_noticias.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9e3bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'FADAT', 'tem', 'como', 'formar', 'indiv√≠duos', 'capazes', 'de', 'buscar', 'conhecimentos', 'e', 'de', 'saber', 'utiliz√°-los', '.', 'Desta', 'forma', ',', 'a', 'Mostra', 'Cient√≠fica', 'da', 'FADAT', 'pretende', 'se', 'tornar', 'tornas', 'um', 'excelente', 'instrumento']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e7cce",
   "metadata": {},
   "source": [
    "### 2. Converting words into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae5d304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7721\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fd80bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('#AltaPerformance', 1)\n",
      "('#AplaudaUmProfessor', 2)\n",
      "('#CompromissoComOsAlunos', 3)\n",
      "('#Faculdade', 4)\n",
      "('#FadatSempreComVoc√™', 5)\n",
      "('#SempreComVoc√™', 6)\n",
      "('#TimeVencedor', 7)\n",
      "('#UnidosPelaVit√≥ria', 8)\n",
      "('#VEMPRAJAOFADAT', 9)\n",
      "('#fadatsemprecomvoc√™', 10)\n",
      "('%', 11)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f64077b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # vocab text -> token IDs\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} # inverse vocab token IDs -> original text tokens\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Process input text into token IDs\"\"\"\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs back into text\"\"\"\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd06cb",
   "metadata": {},
   "source": [
    "### 3. Turning tokens into embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b68016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1599, 2930, 6864, 3596, 6343, 15, 6317, 6602, 6499, 4375, 3113, 6923, 3959, 6201, 5723, 4883, 4244, 3959, 7008, 3612, 6133, 845, 331, 2077, 18]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)\n",
    "text = \"Os candidatos se dedicaram profundamente, produzindo reda√ß√µes que exaltaram com sensibilidade e precis√£o o impacto espiritual e social deixado por Dom Ad√©lio Tomasin.\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f71457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os candidatos se dedicaram profundamente, produzindo reda√ß√µes que exaltaram com sensibilidade e precis√£o o impacto espiritual e social deixado por Dom Ad√©lio Tomasin.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84b53e",
   "metadata": {},
   "source": [
    "**Error if you try to include a word that is not contained on the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98af1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 'candidates'\n"
     ]
    }
   ],
   "source": [
    "text = \"The candidates [...]\"\n",
    "try:\n",
    "    print(tokenizer.decode(tokenizer.encode(text)))\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5155dc7",
   "metadata": {},
   "source": [
    "## Adding special context tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db309e3",
   "metadata": {},
   "source": [
    "Let's modify the above Tokenizer class to include two new tokens, namely `<|unk|>` and `<|endoftext|>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec25e1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7723\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c14e57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('üìà', 7718)\n",
      "('üìë', 7719)\n",
      "('üßë\\u200d‚öñÔ∏è', 7720)\n",
      "('<|endoftext|>', 7721)\n",
      "('<|unk|>', 7722)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7ee6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab # vocab text -> token IDs\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()} # inverse vocab token IDs -> original text tokens\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Process input text into token IDs\"\"\"\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int \n",
    "                        else \"<|unk|>\" for item in preprocessed] # replace unknown words by <|unk|> tokens\n",
    "        \n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs back into text\"\"\"\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # replace spaces before the specified punctuations\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de967bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4167dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7722, 15, 3918, 7722, 7722, 7722, 279, 7721, 7722, 7722, 7722, 7722, 7722, 7722, 7722, 18]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6b8bcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do <|unk|> <|unk|> <|unk|>? <|endoftext|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|> <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a968d8",
   "metadata": {},
   "source": [
    "Some others additional special tokens researchers adopted:\n",
    "\n",
    "- [BOS] (beginning of a sequence)\n",
    "- [EOS] (end of sequence)\n",
    "- [PAD] (padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de31f25",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa897269",
   "metadata": {},
   "source": [
    "BPE breaks down words into subword units, therefore `<|unk|>` becomes inconsequential.\n",
    "\n",
    "It was used to train LLMs such as GPT-2, GPT-3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80b6b340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d633935",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d385bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13027fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dd34262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8c462",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- `<|endoftext|>` token is assigned a relatively large token ID (50256)\n",
    "- BPE tokenizer encodes/decodes unknown words (such as _someunknownPlace_) correctly. \n",
    "\n",
    "This is because BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handler out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd6ac8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901 -> Ak\n",
      "86 -> w\n",
      "343 -> ir\n",
      "86 -> w\n",
      "220 ->  \n",
      "959 -> ier\n",
      "27 -> <\n",
      "1722 -> As\n",
      "67 -> d\n",
      "13 -> .\n",
      "64 -> a\n",
      "29 -> >\n"
     ]
    }
   ],
   "source": [
    "for token in tokenizer.encode(\"Akwirw ier<Asd.a>\"):\n",
    "    print(token, '->' ,tokenizer.decode([token]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
